---
title: "Step 2: The Scone Workflow"
author: "Tyler J Burns"
date: "9/30/2017"
output: html_document
vignette: >
  %\VignetteIndexEntry{General Scone Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = "markup", message = FALSE, warning = FALSE)
```

### K-nearest neighbors:
We read in input.scone.csv, which is our file modified (and renamed) from the get.marker.names() function. The K-nearest neighbor generation is derived from the Fast Nearest Neighbors (FNN) R package, within our function f.nn(), which takes as input the “input markers” to be used, along with the concatenated data previously generated, and the desired k. We advise the default selection to the total number of cells in the dataset divided by 100, as has been optimized on existing mass cytometry datasets. The output of this function is a matrix of each cell and the identity of its k-nearest neighbors, in terms of its row number in the dataset used here as input. 

```{r}
library(Sconify)
```
```{r, eval = FALSE}
# Markers from the user-generated excel file
markers <- parse.markers("input.scone.csv")
```
```{r}
# How to convert your excel sheet into vector of static and functional markers
markers
```

```{r}
# addition in the end gets rid of empty strings
input.markers <- markers[[1]]
funct.markers <- markers[[2]]

# Selection of the k. See "Finding Ideal K" vignette
k <- 30

# The built-in scone functions
wand.nn <- fnn(cell.df = wand.combined, input.markers = input.markers, k = k)
# Cell identity is in rows, k-nearest neighbors are columns
# List of 2 includes the cell identity of each nn, and the euclidean distance between
#   itself and the cell of interest

# Indices
str(wand.nn[[1]])
wand.nn[[1]][1:20, 1:10]

# Distance
str(wand.nn[[2]])
wand.nn[[2]][1:20, 1:10]
```

### Finding scone values:

This function iterates through each KNN, and performs a series of calculations. The first is fold change values for each maker per KNN, where the user chooses whether this will be based on medians or means. The second is a statistical test, where the user chooses t test or Mann-Whitney U test. I prefer the latter, because it does not assume any properties of the distributions. Of note, the p values are adjusted for false discovery rate, and therefore are called q values in the output of this function. The user also inputs a threshold parameter (default 0.05), where the fold change values will only be shown if the corresponding statistical test returns a q value below said threshold. Finally, the "multiple.donor.compare" option, if set to TRUE will perform a t test based on the mean per-marker values of each donor. This is to allow the user to make comparisons across replicates or multiple donors if that is relevant to the user's biological questions. This function returns a matrix of cells by computed values (change and statistical test results, labeled either marker.change or marker.qvalue). This matrix is intermediate, as it gets concatenated with the original input matrix in the post-processing step (see the relevant vignette). We show the code and the output below. See the post-processing vignette, where we show how this gets combined with the input data, and additional analysis is performed. 


```{r, eval = FALSE}
wand.scone <- scone.values(nn.matrix = wand.nn, 
                      cell.data = combined, 
                      scone.markers = funct.markers, 
                      unstim = "basal")
```
```{r}
wand.scone
```

### For programmers: performing additional per-KNN statistics
If one wants to export KNN data to perform other statistics not available in this package, then I provide a 
function that produces a list of each cell identity in the original input data matrix, and a matrix of all 
cells x features of its KNN. 

I also provide a function to find the KNN density estimation independently of the rest of the "scone.values" analysis, to save time if density is all the user wants. With this density estimation, one can perform interesting analysis, ranging from understanding phenotypic density changes along a developmental progression (see post-processing vignette for an example), to trying out density-based binning methods (eg. X-shift). Of note, this density is specifically one divided by the aveage distance to k-nearest neighbors. This specific measure is related to the Shannon Entropy estimate of that point on the manifold (https://hal.archives-ouvertes.fr/hal-01068081/document). 

I use this metric to avoid the unusual properties of the volume of a sphere as it increases in dimensions (https://en.wikipedia.org/wiki/Volume_of_an_n-ball). This being said, one can easily modify this vector to be such a density estimation (example http://www.cs.haifa.ac.il/~rita/ml_course/lectures_old/KNN.pdf), by treating the distance to knn as the radius of a n-dimensional sphere and incoroprating said volume accordingly. 

An individual with basic programming skills can iterate through these elements to perform the statistics of one's choosing. Examples would include per-KNN regression and classification, or feature imputation. The additional functionality is shown below, with the example knn.list in the package being the first ten instances: 

```{r}
# Constructs KNN list, computes KNN density estimation
wand.knn.list <- make.knn.list(cell.data = wand.combined, nn.matrix = wand.nn)
wand.knn.list[[8]]

# Finds the KNN density estimation for each cell, ordered by column, in the original data matrix
wand.knn.density <- get.knn.de(nn.matrix = wand.nn)
str(wand.knn.density)
```


